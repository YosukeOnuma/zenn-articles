---
title: "【初学者向け】VAEをつかむ，式変形はわからなくて良い"
emoji: "😭"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["VAE","生成AI","機械学習"]
published: false
---

## 本記事の狙い
**生成モデル初学者向け**です．VAE（Variational Auto-Encoder）の解説記事を読んでも、式変形ばかりで結局何をしてるのかよくわからない...という人に向けています
- VAEが大まかに何をするか、から解説します
- その後，ネットワーク構造、損失関数など、機械学習のよくあるアプローチで紹介します
- 変分ベイズ推定の式変形は詳しく解説しません

## VAEとは何か
画像などを生成できる，生成モデルです．大まかに以下のことをしています[^1]．
![](/images/vae-without-variational/overview.png)
入力データから本質的な潜在変数を**確率分布として学習する**ため，生成するごとに分布から潜在変数をサンプリングすることで，毎回異なる出力が得られるようになります．
この流れ自体はVAE以外にもよくある話です．VAEの特徴は，学習過程において「変分ベイズ推定」と「Reparameterization trick」を用いることにあります．が，全体像の理解のためにはこれは後回しで良いです．
[^1]: この図の②は若干不正確です．（以下，記事の内容を理解した人向け）潜在変数分布からサンプリングした潜在変数をNNに入れると，出力されるのは顔画像そのものではなく顔画像の分布のパラメタ（平均とか分散とか）であり，つまり$p_\theta (X|z)$です．これで損失関数を計算するのですからね．ただ実用上，画像を生成する際は，この顔画像分布から再びサンプリングするのではなく，その平均値を生成結果とすることが多いらしいです．

## ネットワークの構造
VAEは下図のように，EncoderとDecoderの２つのニューラルネットワークで構成されています．先ほどの図の③がEncoder，②がDecoderですね．
![](/images/vae-without-variational/structure.png)
図中の"sample"というのは，確率分布に従って変数をランダムに生成する，ということです．
なおわかりやすさのために，図中ではDecoderが出力する$z$のパラメタは$\mu$,$\sigma$の２種類になっています．しかし実務では$\sigma$は固定値を用いたり，そもそも正規分布以外の分布を採用したりなど，様々です．

:::message
**学習が済めば，画像の生成に必要なのは，学習した潜在変数$z$のパラメタとDecoderNNだけ**です．EncoderNNは，潜在変数$z$のパラメタを得るために学習時のみ使用します．
:::

:::message
最初の図ではDecoderの出力が「画像そのもの」かのように書きましたが，厳密にはこの図のように出力は「画像の確率分布」になります．ただ実務上はこの分布からはサンプリングせず，分布の平均値（などの代表値）をそのまま出力とすることが多いようで，「画像を出力している」とも言えるかなと思います．
:::

## 学習の流れ

ミニバッチごとに、以下の処理を繰り返しています．

1. データを生成
    - 入力データ$X$をEncoderに通し，潜在変数$z$の分布を得る
    - $z$を得られた分布に従ってサンプリング
    - その$z$をDecoderに通して生成データ$X'$の分布を得る
2. 損失計算，そして誤差逆伝播
    - $X$，$X'$の分布，そして$z$の分布から損失関数（後述）を計算
    - この損失についてDecoderNN，EncoderNNをさかのぼり誤差逆伝播を行う
    - （SGDやAdamなどで）両ネットワークのパラメータを一緒に更新

「サンプリング」という確率的な操作があるのに，Encoder側まで一気に誤差逆伝播ができます．いったいどうやって？というのがReparameterization trickです．後ほど説明します．

## 損失関数
VAEではELBOと呼ばれる次の値を**最大化**することを考えます。**第一項が再構成項（生成の正確さ）、第二項が正則化項**です．
    
$$
\text{ELBO} = \mathbb{E}_{q_\phi (z|X)}[\text{log}\,p_\theta (X|z)] - \text{KL}\bigl( q_\phi (z|X)|| p_{prior}(z) \bigr)
$$

まず分布の記号を確認していきましょう
- $p_\theta (X|z)$ ... **DecoderNNが表す分布です**．つまり$z$を与えると出力される，生成データの分布のことです．$\theta$はネットワークのパラメタです．
- $q_\phi (z|X)$ ... **EncoderNNが表す分布です**．つまり$X$を与えると出力される，潜在変数の分布のことです．$p$ではなく$q$を使うのは，これが変分ベイズ推定における「近似事後分布」を意味するからです．
- $p_{prior}(z)$ ... EncoderNNで$z$の分布をベイズ推定する上での事前分布です．VAEでは平均0，分散1の標準正規分布を基本的に用います．

続いて操作の記号を整理します
- $\mathbb{E}_{q_\phi (z|X)}(\,\cdot\,)$ ... $q$という$z$の分布に沿った期待値，ということです
- $\text{KL}(\,\cdot\, || \,\cdot\,)$ ... 2つの分布の間のKLダイバージェンス（距離のようなもの）です．

最後に，各項の簡単な説明です．
    
- 第１項を大きくすることは、「出力$X'$の期待値が入力$X$に近づく」こと，つまりより入力が再現できています．
- 第２項のKLを小さくすることは、「Encoderによる$z$の分布が事前分布に近づく」こと，これで過学習を防ぎます．

もちろんこの２項のバランスはそう単純じゃないので、第二項に重みをつけたりします


## （余談）Auto Encoderとの違い
- Encoderでデータから潜在変数を学習し，Decoderで潜在変数からデータの再生成を試みるため，結果的にAuto Encoderの形になっています．AEは入力をそのまま再現することだけを考えますが、VAEは潜在変数$z$の分布が標準正規分布（など設定した事前分布）に近いと想定しながら、入力$X$に整合的な潜在空間$z$の**分布**を学習します


## Reparametrization trick

- ニューラルネットの中に確率過程があるため、普通の誤差逆伝播ができない。ここで、VAEの構造を以下のように解釈することで、確率過程を外に出して、誤差逆伝播を可能にした。これをreparametrization trickと言う。（そんな突飛でもねえな）
$z=μ_ϕ(X)+σ_ϕ(X)ϵ \; (\,ϵ∼N(0,I)\,)$ をサンプリング（後述のreparametrization）

## 損失関数の導出 - 変分ベイズ推定の部分

## 終わりに
VAEは実際のところ、変分ベイズ推定による潜在変数$z$の推定から誕生した手法です。ですがここまで見てきたように、結果的に生まれた損失関数やネットワーク構造からでも、十分何が起きているのか理解できたかと思います。

ここまで読んだ上で、これがどのように変分ベイズ推定に基づいているのか興味が湧いた方は、そこを深掘っている記事を読みあさっていただければと思います。

## 特に参考にしたサイト
- [Variational Autoencoder徹底解説](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24)
- [VAEって結局何者なの？](https://zenn.dev/asap/articles/6caa9043276424#vae（variational-auto-encoder）)
- 画像資料の一部は生成AIを用いています
